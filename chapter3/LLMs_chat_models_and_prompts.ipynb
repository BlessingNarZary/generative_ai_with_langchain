{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053cd6ee-c227-475b-8a56-b44a76ac1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce224750-7b63-4881-a631-cf1905ce7333",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222e2d0a-4336-4cf2-95c5-3fbda0e65bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0daa7d-671e-4366-8a9c-a6e46f2891b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import AnthropicLLM\n",
    "\n",
    "llm = AnthropicLLM(model='claude-2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440201ee-076a-4c12-a1a4-a00ab3e50a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = llm.invoke(\"Tell me a joke about light bulbs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9485c062-2c00-4d8e-8418-1a753c2a5165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the light bulb go to therapy?\n",
      "\n",
      "Because it was feeling a little dim!\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5df09b0-4026-4188-8bec-d0135a79fb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/anaconda3/envs/newgen/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n",
      "/Users/ben/anaconda3/envs/newgen/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japan\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n",
    "    repo_id=\"google/flan-t5-xxl\"\n",
    ")\n",
    "prompt = \"In which country is Tokyo?\"\n",
    "completion = llm(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e25a58b-78d7-44a9-a225-052446b0f7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.fake import FakeListLLM\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "fake_llm.invoke(\"Hi and goodbye, FakeListLLM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da013d9c-516e-44bb-bc50-a015e4d1153c",
   "metadata": {},
   "source": [
    "# Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0572810-d161-478a-b899-bc73f5c4f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='print(\"Hello world\")' response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 15, 'total_tokens': 20}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-0613')\n",
    "response = llm.invoke([HumanMessage(content='Say \"Hello world\" in Python.')])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b996784-19dc-4f18-a739-b7026d1fa4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Model regularization is a technique used to prevent overfitting in machine learning models. Overfitting happens when a model learns too much from the training data, including noise and outliers, and performs poorly on unseen data.\\n\\nRegularization works by adding a penalty term to the loss function, which discourages the learning algorithm from assigning too much importance to any individual predictor (feature). This effectively reduces the complexity of the model, making it more generalizable to unseen data.\\n\\nThere are different types of regularization techniques, including L1 regularization (Lasso regression), L2 regularization (Ridge regression), and a combination of both (Elastic net). \\n\\nIn summary, the purpose of model regularization is to improve the generalization capability of a model, reduce model complexity, and prevent overfitting.' response_metadata={'token_usage': {'completion_tokens': 157, 'prompt_tokens': 24, 'total_tokens': 181}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "\n",
    "chat_output = llm.invoke([ \n",
    "    SystemMessage(content=\"You're a helpful assistant\"), \n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\") \n",
    "])\n",
    "print(chat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a6c01-43c6-4b09-9046-dd09074f42c7",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c22d0008-ec11-4032-838c-ee384f9010c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Summarize this text in one sentence:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI()\n",
    "summary = llm(prompt.format(text=\"Some long story\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11210a79-5f4b-4f8b-b6da-0902b9a32a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The text is a long story.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cd7b20f-e641-4aec-96a8-c8ccfe19dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eef134d-c7ac-4381-8fe6-da21ac49b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ae58429-c786-4f39-8ae0-c876ed7b3b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98b851e-1da0-4810-b23e-5fc1b409166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b71caef-6c52-4c11-9b14-dd52384c7b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a funny joke about chickens.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e777186-669f-4aba-832a-72a90b2cd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# Define a ChatPromptTemplate to translate text\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are an English to French translator.'),\n",
    "    ('user', 'Translate this to French: {text}')\n",
    "])\n",
    "llm = ChatOpenAI()\n",
    "# Translate a joke about light bulbs\n",
    "response = llm.invoke(template.format_messages(text='How many programmers does it take to change a light bulb?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6edd9b38-80de-426b-8080-6bb2b0169292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Combien de programmeurs faut-il pour changer une ampoule?' response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 36, 'total_tokens': 50}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4e4ee06-2648-40ed-bafc-2d60b573abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What NFL team won the Super Bowl in the year Justin Beiber was born?\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Step 1: Find the year Justin Bieber was born.**\\n\\nJustin Bieber was born on March 1, 1994.\\n\\n**Step 2: Determine the NFL season that began in 1993 and ended in 1994.**\\n\\nThis would be the 1993 NFL season.\\n\\n**Step 3: Find out which NFL team won the Super Bowl at the end of the 1993 NFL season.**\\n\\nThe Dallas Cowboys won Super Bowl XXVIII at the end of the 1993 NFL season.\\n\\n**Answer: Dallas Cowboys**'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "#from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "llm = ChatVertexAI(model_name=\"gemini-pro\")\n",
    "#llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d83a0f-5788-46f6-a5ef-b83fe0b9c4cf",
   "metadata": {},
   "source": [
    "# LangChain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2013137-d9ab-434e-8256-d2983c883ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/ben/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.schema import StrOutputParser\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, max_length=128, temperature=0.5,\n",
    ")\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\" \n",
    "prompt = PromptTemplate.from_template(template) \n",
    "runnable = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9167ee3c-e39a-40cc-9bbd-446ab5c37b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \" \n",
    "summary = runnable.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3187ecc-b88b-464b-b19c-504ec504bb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The FIFA World Cup is an international football competition. It is held every four years. So, to find out which team won the FIFA World Cup in 1994, we need to identify which World Cup tournament that was.\n",
      "\n",
      "First, we know that the first FIFA World Cup was held in 1930. Since then, the tournament has been held every four years, except for 1942 and 1946 due to World War II.\n",
      "\n",
      "To calculate which World Cup tournament was held in 1994, we can subtract the year of the first World Cup (1930) from 1994 and divide by 4. We'll round down to the nearest whole number because the tournament is held in the year indicated, not the year before.\n",
      "\n",
      "1994 - 1930 = 64\n",
      "64 / 4 = 16.025\n",
      "\n",
      "Since we can't have a fraction of a tournament, we round down to 16. That means there have been 16 World Cup tournaments as of 1994. The most recent one before 1994 was in 1990. So, the FIFA World Cup in 1994 would be the one following that, which was won by Brazil.\n",
      "\n",
      "Therefore, the answer is: Brazil won the FIFA World Cup in 1994.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52c272-3bb7-429f-aac5-0bfeba757c95",
   "metadata": {},
   "source": [
    "# Decorator syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ea57f58-40b8-4b14-9aab-78f4ba76db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def summarize(topic: str, manner: str = \"fun\") -> str:\n",
    "    \"\"\"\n",
    "    Write a short paragraph in {manner} fashion about {topic}\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "summary = summarize(topic=\"The history of Python is both interesting and entertaining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11c63f95-1058-4ca6-ad74-3063576e6f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history of Python is like a wild and wacky adventure through the coding jungle. It all started back in the late 1980s when a brilliant programmer named Guido van Rossum decided to create a language that was easy to read and fun to use. As Python evolved, it gained a cult following of developers who were drawn to its simplicity and versatility. With its quirky name and playful spirit, Python has become a beloved language in the world of programming. So, grab your snake charmer and get ready to slither through the fascinating history of Python!\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453b707-2b5e-41d3-9913-d8a44cd2b802",
   "metadata": {},
   "source": [
    "# Text-To-Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68ecf7e9-9182-4acf-b701-7c63537e33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper \n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9) \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"image_desc\"],\n",
    "    template=(\n",
    "        \"Generate a concise prompt to generate an image based on the following description: \"\n",
    "        \"{image_desc}\"\n",
    "))\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c35bd79-7ece-496b-a426-925a14ca0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = chain.run(\"halloween night at a haunted museum\")\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e869d8c6-1dd4-48cd-8c03-53a96b983263",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = DallEAPIWrapper().run(chain.run(\"halloween night at a haunted museum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e74293b-9030-4193-b0b9-8a7d61d559d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-Pn9wx3ImDdNoHSJaKR9gYmHu.png?st=2024-04-04T08%3A15%3A26Z&se=2024-04-04T10%3A15%3A26Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-04-03T22%3A36%3A11Z&ske=2024-04-04T22%3A36%3A11Z&sks=b&skv=2021-08-06&sig=tHlsFm990prI/PuqtgmjgIsr8UYkYGoEFw08sHrT7/Q%3D'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee924758-e05b-4e1c-8f7c-b97e03c1a8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-Pn9wx3ImDdNoHSJaKR9gYmHu.png?st=2024-04-04T08%3A15%3A26Z&se=2024-04-04T10%3A15%3A26Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-04-03T22%3A36%3A11Z&ske=2024-04-04T22%3A36%3A11Z&sks=b&skv=2021-08-06&sig=tHlsFm990prI/PuqtgmjgIsr8UYkYGoEFw08sHrT7/Q%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71bff935-dbce-4ba1-b6d9-40cf9df21eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(image_url)\n",
    "image_path = \"haunted_house.png\"\n",
    "with open(image_path, \"wb\") as f:\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67128851-c0f2-4b0e-9374-562ca0d5b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "text2image = Replicate(\n",
    "    model=(\n",
    "        \"stability-ai/stable-diffusion:\"\n",
    "        \"d70beb400d223e6432425a5299910329c6050c6abcf97b8c70537d6a1fcb269a\"\n",
    "    ),\n",
    "    model_kwargs={\"image_dimensions\": \"512x512\"}\n",
    ")\n",
    "image_url = text2image(\"a book cover for a book about creating generative ai applications in Python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf684106-fdca-48a9-a546-a152ea8bef20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://replicate.delivery/pbxt/vASFAcXFpEq5EdeRuixdexDvRrcUNqMai1XXpPk8XMDoUCnSA/out-0.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b31e0502-aa93-40e5-b3d8-bd35208d1fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"This image appears to be a digital or stylized representation of an object or pattern, possibly intended to be abstract or artistic in nature. The central figure is a hexagon-like shape made up of smaller hexagons in a blue color, with yellow dots placed at the vertices of the smaller hexagons. The background is a vibrant orange with a green circular border, and the whole image is overlaid with a texture that gives it a somewhat crackled or mosaic appearance.\\n\\nAdditionally, there is text overlaying the image in a script that is difficult to read due to the decorative font and the complex background. The text doesn't seem to correspond to any recognizable language and may be purely aesthetic or fictional. The image might be a cover for a music album, a piece of artwork, or a representation of something from a specific subculture or fandom, but without more context, it's hard to determine its exact purpose or origin.\", response_metadata={'token_usage': {'completion_tokens': 186, 'prompt_tokens': 267, 'total_tokens': 453}, 'model_name': 'gpt-4-vision-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage \n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=256)\n",
    "chat.invoke([\n",
    "    HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"What is this image showing\"}, \n",
    "            {\n",
    "                \"type\": \"image_url\", \n",
    "                \"image_url\": { \"url\": image_url, \"detail\": \"auto\", },\n",
    "            }, \n",
    "        ]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe8dc40c-2837-4ebe-9a92-d171cf8a9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_image = \"\"\"\n",
    "The image appears to be a diagram representing the architecture or components of a software system or framework related to language processing, possibly named LangChain or associated with a project or product called LangChain, based on the prominent appearance of that term. The diagram is organized into several layers or aspects, each containing various elements or modules:\\n\\n1. **Protocol**: This may be the foundational layer, which includes \"LCEL\" and terms like parallelization, fallbacks, tracing, batching, streaming, async, and composition. These seem related to communication and execution protocols for the system.\\n\\n2. **Integrations Components**: This layer includes \"Model I/O\" with elements such as the model, output parser, prompt, and example selector. It also has a \"Retrieval\" section with a document loader, retriever, embedding model, vector store, and text splitter. Lastly, there\\'s an \"Agent Tooling\" section. These components likely deal with the interaction with external data, models, and tools.\\n\\n3. **Application**: The application layer features \"LangChain\" with chains, agents, agent executors, and common application logic. This suggests that the system uses a modular approach with chains and agents to process language tasks.\\n\\n4. **Deployment**: This contains \"Lang'\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "912d0ceb-4545-445a-8fce-5492b195619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Design a software system or framework for language processing called LangChain, with layers for protocol, integration components, application, and deployment. The system uses chains and agents for processing and includes components for communication protocols, external data interaction, and common application logic.\"\n"
     ]
    }
   ],
   "source": [
    "# prompt = chain.run(\"halloween night at a haunted museum\")\n",
    "# print(prompt)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"image_desc\"],\n",
    "    template=(\n",
    "        \"Simplify this image description into a concise prompt to generate an image: \"\n",
    "        \"{image_desc}\"\n",
    "))\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "prompt = chain.run(langchain_image)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e7a9686-2945-4845-a126-c4a8b45a6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = DallEAPIWrapper().run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "497caa44-baf3-49b3-b59e-0eb2ef98ba24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-8M4zAKpCe5UxgqgeXylYde3z.png?st=2024-04-04T08%3A38%3A34Z&se=2024-04-04T10%3A38%3A34Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-04-03T11%3A33%3A47Z&ske=2024-04-04T11%3A33%3A47Z&sks=b&skv=2021-08-06&sig=hC/oQDtMQkl3so3D2o6l7QaZx1gLjm2ie%2BiE/ctwQNQ%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793becd1-52d8-406f-9694-32c0f3dbbdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
