{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "053cd6ee-c227-475b-8a56-b44a76ac1ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:18:58.105392Z",
     "start_time": "2024-08-04T18:18:58.079978Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting the environment variables\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce224750-7b63-4881-a631-cf1905ce7333",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222e2d0a-4336-4cf2-95c5-3fbda0e65bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary = llm.invoke(\"Tell me a joke about light bulbs!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the light bulb go to therapy?\n",
      "\n",
      "Because it was feeling a little dim!\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5df09b0-4026-4188-8bec-d0135a79fb80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:16:19.836563Z",
     "start_time": "2024-08-04T18:15:32.449845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/anaconda3/envs/softupdate/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "504 Server Error: Gateway Timeout for url: https://api-inference.huggingface.co/models/google/flan-t5-xxl (Request ID: b45mEAM3hrF7cghCacl0a)\n\nModel google/flan-t5-xxl time out",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 504 Server Error: Gateway Timeout for url: https://api-inference.huggingface.co/models/google/flan-t5-xxl",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 7\u001B[0m\n\u001B[1;32m      2\u001B[0m llm \u001B[38;5;241m=\u001B[39m HuggingFaceHub(\n\u001B[1;32m      3\u001B[0m     model_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m64\u001B[39m},\n\u001B[1;32m      4\u001B[0m     repo_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle/flan-t5-xxl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m )\n\u001B[1;32m      6\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn which country is Tokyo?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 7\u001B[0m completion \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(completion)\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/langchain_core/language_models/llms.py:276\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    273\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    274\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    275\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 276\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    288\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/langchain_core/language_models/llms.py:597\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    590\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    591\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    594\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    595\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    596\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 597\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/langchain_core/language_models/llms.py:767\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    753\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    754\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    755\u001B[0m             dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    765\u001B[0m         )\n\u001B[1;32m    766\u001B[0m     ]\n\u001B[0;32m--> 767\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    770\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    771\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/langchain_core/language_models/llms.py:634\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[1;32m    633\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 634\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    635\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/langchain_core/language_models/llms.py:621\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    611\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    612\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    613\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    618\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    619\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    620\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 621\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    625\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    628\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    629\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    630\u001B[0m         )\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1231\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1228\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1229\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m   1230\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m-> 1231\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1232\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m   1233\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1234\u001B[0m     )\n\u001B[1;32m   1235\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[1;32m   1236\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/langchain_community/llms/huggingface_hub.py:135\u001B[0m, in \u001B[0;36mHuggingFaceHub._call\u001B[0;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    132\u001B[0m _model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    133\u001B[0m parameters \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_model_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m--> 135\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m response \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m response:\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:304\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/anaconda3/envs/softupdate/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:371\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HfHubHTTPError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[0;32m--> 371\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HfHubHTTPError(\u001B[38;5;28mstr\u001B[39m(e), response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m: 504 Server Error: Gateway Timeout for url: https://api-inference.huggingface.co/models/google/flan-t5-xxl (Request ID: b45mEAM3hrF7cghCacl0a)\n\nModel google/flan-t5-xxl time out"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n",
    "    repo_id=\"google/flan-t5-xxl\"\n",
    ")\n",
    "prompt = \"In which country is Tokyo?\"\n",
    "completion = llm.invoke(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e25a58b-78d7-44a9-a225-052446b0f7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.fake import FakeListLLM\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "fake_llm.invoke(\"Hi and goodbye, FakeListLLM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da013d9c-516e-44bb-bc50-a015e4d1153c",
   "metadata": {},
   "source": [
    "# Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0572810-d161-478a-b899-bc73f5c4f032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:11:12.418721Z",
     "start_time": "2024-08-04T18:11:07.728116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='print(\"Hello world\")' response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 15, 'total_tokens': 20}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1dcc66b3-7566-43f0-92e7-340b701abc45-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-0613')\n",
    "response = llm.invoke([HumanMessage(content='Say \"Hello world\" in Python.')])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b996784-19dc-4f18-a739-b7026d1fa4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:11:31.022597Z",
     "start_time": "2024-08-04T18:11:25.411463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Model regularization is a technique used to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data so well that it performs poorly on new, unseen data. This is because it has essentially memorized the training data, including its noise and outliers, rather than learning the underlying patterns.\\n\\nRegularization adds a penalty on the complexity of the model, discouraging learning a more complex model. In other words, it introduces some bias into the model to make it less sensitive to the training data and thus more generalizable to new data. This is achieved by adding a penalty term to the loss function that the model is trying to minimize.\\n\\nThere are different types of regularization techniques, such as L1 (Lasso), L2 (Ridge), and Elastic Net, each with their own way of adding a penalty to the loss function.' response_metadata={'token_usage': {'completion_tokens': 173, 'prompt_tokens': 24, 'total_tokens': 197}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-af1361d4-c415-4b2e-ba4b-5328b7413d66-0'\n",
      "content='Model regularization is used in machine learning to prevent overfitting. Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model.\\n\\nRegularization introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce the complexity of the model, makes the model simpler, improves its generalizability, and thus reduces the risk of overfitting. \\n\\nCommon types of regularization techniques include Ridge Regression, Lasso Regression, and Elastic Net.' response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 24, 'total_tokens': 176}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-dd4bfe81-a471-47b8-9e8e-2e68ed125f62-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_output = llm.invoke([ \n",
    "    SystemMessage(content=\"You're a helpful assistant\"), \n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\") \n",
    "])\n",
    "print(chat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model='claude-3-opus-20240229',\n",
    ")\n",
    "\n",
    "response = llm.invoke([HumanMessage(content='What is the best large language model?')])\n",
    "\n",
    "print(response)\n",
    "# please note that this needs a sufficient credit balance!"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T19:10:51.393875Z",
     "start_time": "2024-08-04T19:10:50.923029Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "6a4a6c01-43c6-4b09-9046-dd09074f42c7",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c22d0008-ec11-4032-838c-ee384f9010c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Summarize this text in one sentence:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI()\n",
    "summary = llm(prompt.format(text=\"Some long story\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11210a79-5f4b-4f8b-b6da-0902b9a32a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The text is a long story.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cd7b20f-e641-4aec-96a8-c8ccfe19dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eef134d-c7ac-4381-8fe6-da21ac49b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ae58429-c786-4f39-8ae0-c876ed7b3b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98b851e-1da0-4810-b23e-5fc1b409166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b71caef-6c52-4c11-9b14-dd52384c7b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a funny joke about chickens.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e777186-669f-4aba-832a-72a90b2cd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# Define a ChatPromptTemplate to translate text\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are an English to French translator.'),\n",
    "    ('user', 'Translate this to French: {text}')\n",
    "])\n",
    "llm = ChatOpenAI()\n",
    "# Translate a joke about light bulbs\n",
    "response = llm.invoke(template.format_messages(text='How many programmers does it take to change a light bulb?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6edd9b38-80de-426b-8080-6bb2b0169292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Combien de programmeurs faut-il pour changer une ampoule?' response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 36, 'total_tokens': 50}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4e4ee06-2648-40ed-bafc-2d60b573abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mQuestion: What NFL team won the Super Bowl in the year Justin Beiber was born?\n",
      "Answer: Let's think step by step.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Step 1: Find the year Justin Bieber was born.**\\n\\nJustin Bieber was born on March 1, 1994.\\n\\n**Step 2: Determine the NFL season that began in 1993 and ended in 1994.**\\n\\nThis would be the 1993 NFL season.\\n\\n**Step 3: Find out which NFL team won the Super Bowl at the end of the 1993 NFL season.**\\n\\nThe Dallas Cowboys won Super Bowl XXVIII at the end of the 1993 NFL season.\\n\\n**Answer: Dallas Cowboys**'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "#from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "llm = ChatVertexAI(model_name=\"gemini-pro\")\n",
    "#llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d83a0f-5788-46f6-a5ef-b83fe0b9c4cf",
   "metadata": {},
   "source": [
    "# LangChain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2013137-d9ab-434e-8256-d2983c883ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/ben/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.schema import StrOutputParser\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, max_length=128, temperature=0.5,\n",
    ")\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\" \n",
    "prompt = PromptTemplate.from_template(template) \n",
    "runnable = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9167ee3c-e39a-40cc-9bbd-446ab5c37b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \" \n",
    "summary = runnable.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3187ecc-b88b-464b-b19c-504ec504bb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The FIFA World Cup is an international football competition. It is held every four years. So, to find out which team won the FIFA World Cup in 1994, we need to identify which World Cup tournament that was.\n",
      "\n",
      "First, we know that the first FIFA World Cup was held in 1930. Since then, the tournament has been held every four years, except for 1942 and 1946 due to World War II.\n",
      "\n",
      "To calculate which World Cup tournament was held in 1994, we can subtract the year of the first World Cup (1930) from 1994 and divide by 4. We'll round down to the nearest whole number because the tournament is held in the year indicated, not the year before.\n",
      "\n",
      "1994 - 1930 = 64\n",
      "64 / 4 = 16.025\n",
      "\n",
      "Since we can't have a fraction of a tournament, we round down to 16. That means there have been 16 World Cup tournaments as of 1994. The most recent one before 1994 was in 1990. So, the FIFA World Cup in 1994 would be the one following that, which was won by Brazil.\n",
      "\n",
      "Therefore, the answer is: Brazil won the FIFA World Cup in 1994.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453b707-2b5e-41d3-9913-d8a44cd2b802",
   "metadata": {},
   "source": [
    "# Text-To-Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68ecf7e9-9182-4acf-b701-7c63537e33e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:41:46.341687Z",
     "start_time": "2024-08-04T18:41:46.195502Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper \n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9) \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"image_desc\"],\n",
    "    template=(\n",
    "        \"Generate a concise prompt to generate an image based on the following description: \"\n",
    "        \"{image_desc}\"\n",
    "))\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c35bd79-7ece-496b-a426-925a14ca0fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:41:46.962590Z",
     "start_time": "2024-08-04T18:41:46.955463Z"
    }
   },
   "outputs": [],
   "source": [
    "# prompt = chain.run(\"halloween night at a haunted museum\")\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e869d8c6-1dd4-48cd-8c03-53a96b983263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:42:31.032762Z",
     "start_time": "2024-08-04T18:42:21.330833Z"
    }
   },
   "outputs": [],
   "source": [
    "image_url = DallEAPIWrapper().run(chain.run(\"halloween night at a haunted museum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e74293b-9030-4193-b0b9-8a7d61d559d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:42:31.046947Z",
     "start_time": "2024-08-04T18:42:31.037518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-d6ZCx9l2RS0l4hNPGKd4cFM3.png?st=2024-08-04T17%3A42%3A31Z&se=2024-08-04T19%3A42%3A31Z&sp=r&sv=2023-11-03&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-08-04T10%3A58%3A29Z&ske=2024-08-05T10%3A58%3A29Z&sks=b&skv=2023-11-03&sig=tTqUdrsZ4FZqY5MKlYkv7iTIhUR4at1GpP9spKrBUfc%3D'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee924758-e05b-4e1c-8f7c-b97e03c1a8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:42:31.075994Z",
     "start_time": "2024-08-04T18:42:31.050766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-d6ZCx9l2RS0l4hNPGKd4cFM3.png?st=2024-08-04T17%3A42%3A31Z&se=2024-08-04T19%3A42%3A31Z&sp=r&sv=2023-11-03&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-08-04T10%3A58%3A29Z&ske=2024-08-05T10%3A58%3A29Z&sks=b&skv=2023-11-03&sig=tTqUdrsZ4FZqY5MKlYkv7iTIhUR4at1GpP9spKrBUfc%3D\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71bff935-dbce-4ba1-b6d9-40cf9df21eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:42:35.542285Z",
     "start_time": "2024-08-04T18:42:31.223652Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(image_url)\n",
    "image_path = \"haunted_house.png\"\n",
    "with open(image_path, \"wb\") as f:\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67128851-c0f2-4b0e-9374-562ca0d5b688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:44:19.854858Z",
     "start_time": "2024-08-04T18:43:58.435512Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "text2image = Replicate(\n",
    "    model=(\n",
    "        \"stability-ai/stable-diffusion:\"\n",
    "        \"27b93a2413e7f36cd83da926f3656280b2931564ff050bf9575f1fdf9bcd7478\"\n",
    "    ),\n",
    "    model_kwargs={\"image_dimensions\": \"512x512\"}\n",
    ")\n",
    "image_url = text2image(\"a book cover for a book about creating generative ai applications in Python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf684106-fdca-48a9-a546-a152ea8bef20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://replicate.delivery/pbxt/vASFAcXFpEq5EdeRuixdexDvRrcUNqMai1XXpPk8XMDoUCnSA/out-0.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b31e0502-aa93-40e5-b3d8-bd35208d1fac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:48:12.246305Z",
     "start_time": "2024-08-04T18:48:07.149559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='This image appears to be an intricate digital art piece featuring a symmetrical, kaleidoscopic design. The design incorporates vivid neon colors, predominantly in shades of green and yellow. The pattern is detailed and complex, featuring interconnected geometric shapes and motifs that resemble webs or circuits. The words \"EDYOOK AND INNOVATION\" are visible at the top of the image, suggesting the image might be related to a theme of education and innovation, or it could be a logo or promotional graphic for a project or organization under that name.', response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 267, 'total_tokens': 375}, 'model_name': 'gpt-4-turbo', 'system_fingerprint': 'fp_0e7b71926a', 'finish_reason': 'stop', 'logprobs': None}, id='run-41a83844-fb07-4d72-879c-9b995a5ac123-0')"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(model=\"gpt-4-turbo\", max_tokens=256)\n",
    "chat.invoke([\n",
    "    HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"What is this image showing\"}, \n",
    "            {\n",
    "                \"type\": \"image_url\", \n",
    "                \"image_url\": { \"url\": image_url, \"detail\": \"auto\", },\n",
    "            }, \n",
    "        ]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe8dc40c-2837-4ebe-9a92-d171cf8a9241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:51:11.283848Z",
     "start_time": "2024-08-04T18:51:11.272390Z"
    }
   },
   "outputs": [],
   "source": [
    "langchain_image = \"\"\"\n",
    "The image appears to be a diagram representing the architecture or components of a software system or framework related to language processing, possibly named LangChain or associated with a project or product called LangChain, based on the prominent appearance of that term. The diagram is organized into several layers or aspects, each containing various elements or modules:\\n\\n1. **Protocol**: This may be the foundational layer, which includes \"LCEL\" and terms like parallelization, fallbacks, tracing, batching, streaming, async, and composition. These seem related to communication and execution protocols for the system.\\n\\n2. **Integrations Components**: This layer includes \"Model I/O\" with elements such as the model, output parser, prompt, and example selector. It also has a \"Retrieval\" section with a document loader, retriever, embedding model, vector store, and text splitter. Lastly, there\\'s an \"Agent Tooling\" section. These components likely deal with the interaction with external data, models, and tools.\\n\\n3. **Application**: The application layer features \"LangChain\" with chains, agents, agent executors, and common application logic. This suggests that the system uses a modular approach with chains and agents to process language tasks.\\n\\n4. **Deployment**: This contains \"Lang'\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "912d0ceb-4545-445a-8fce-5492b195619d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:51:12.878508Z",
     "start_time": "2024-08-04T18:51:11.773666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Create an image of the architecture and components of a language processing system called LangChain, featuring layers for protocol, integration components, application, and deployment, with visual representations of elements such as chains, agents, and common application logic.\"\n"
     ]
    }
   ],
   "source": [
    "# prompt = chain.run(\"halloween night at a haunted museum\")\n",
    "# print(prompt)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"image_desc\"],\n",
    "    template=(\n",
    "        \"Simplify this image description into a concise prompt to generate an image: \"\n",
    "        \"{image_desc}\"\n",
    "))\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "prompt = chain.run(langchain_image)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e7a9686-2945-4845-a126-c4a8b45a6a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:51:24.768435Z",
     "start_time": "2024-08-04T18:51:12.884831Z"
    }
   },
   "outputs": [],
   "source": [
    "image_url = DallEAPIWrapper().run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "497caa44-baf3-49b3-b59e-0eb2ef98ba24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:51:24.770635Z",
     "start_time": "2024-08-04T18:51:24.751050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-j3H4ZdLSXgJXd0fMc2lf3ttZ.png?st=2024-08-04T17%3A51%3A24Z&se=2024-08-04T19%3A51%3A24Z&sp=r&sv=2023-11-03&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-08-04T14%3A47%3A09Z&ske=2024-08-05T14%3A47%3A09Z&sks=b&skv=2023-11-03&sig=Ryd%2Bajr7YCOoCaMJIyN3tXxFPns7%2BRCnBiyBvopbaYk%3D\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793becd1-52d8-406f-9694-32c0f3dbbdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
